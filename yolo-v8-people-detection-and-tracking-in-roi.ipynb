{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-03-06T19:20:48.240907Z",
     "iopub.status.busy": "2023-03-06T19:20:48.240227Z",
     "iopub.status.idle": "2023-03-06T19:21:12.542489Z",
     "shell.execute_reply": "2023-03-06T19:21:12.541302Z",
     "shell.execute_reply.started": "2023-03-06T19:20:48.24087Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install ultralytics -qq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.26.4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy==1.26.4 in c:\\users\\sudip\\.conda\\envs\\yolo\\lib\\site-packages (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy==1.26.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-06T19:21:12.547009Z",
     "iopub.status.busy": "2023-03-06T19:21:12.546688Z",
     "iopub.status.idle": "2023-03-06T19:21:15.013655Z",
     "shell.execute_reply": "2023-03-06T19:21:15.012581Z",
     "shell.execute_reply.started": "2023-03-06T19:21:12.546981Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Object Detecion \n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "#plots\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#basics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Display image and videos\n",
    "import IPython\n",
    "from IPython.display import Video, display\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import urllib.request \n",
    "import shutil\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A People Detection and Counting project in a ROI based on the Yolo V8 Model.\n",
    "----------------------\n",
    " **The objectives of the project are:**\n",
    "- To detect people that are passing in a Region of interest (ROI) \n",
    "- Track each individual with a unique ID in the ROI \n",
    "\n",
    "---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\PROJECT_ON_Github\\Opencv_Smart_Door\n"
     ]
    }
   ],
   "source": [
    "!cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-06T19:21:15.01601Z",
     "iopub.status.busy": "2023-03-06T19:21:15.015084Z",
     "iopub.status.idle": "2023-03-06T19:21:15.784191Z",
     "shell.execute_reply": "2023-03-06T19:21:15.783171Z",
     "shell.execute_reply.started": "2023-03-06T19:21:15.015972Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Video  path for experiment\n",
    "path_zip = 'https://github.com/freedomwebtech/roiinyolo/raw/main/vid1.zip' # credits to the github repo for the video\n",
    "urllib.request.urlretrieve(path_zip, \"vid1.zip\")\n",
    "shutil.unpack_archive('vid1.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'D:\\\\PROJECT_ON_Github\\\\Opencv_Smart_Door\\\\vid1.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-06T19:21:15.787135Z",
     "iopub.status.busy": "2023-03-06T19:21:15.786672Z",
     "iopub.status.idle": "2023-03-06T19:21:25.107752Z",
     "shell.execute_reply": "2023-03-06T19:21:25.106758Z",
     "shell.execute_reply.started": "2023-03-06T19:21:15.787099Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#loading a YOLO model \n",
    "model = YOLO('yolov8x.pt')\n",
    "\n",
    "#geting names from classes\n",
    "dict_classes = model.model.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-06T19:21:25.109901Z",
     "iopub.status.busy": "2023-03-06T19:21:25.109225Z",
     "iopub.status.idle": "2023-03-06T19:21:25.122303Z",
     "shell.execute_reply": "2023-03-06T19:21:25.121348Z",
     "shell.execute_reply.started": "2023-03-06T19:21:25.109862Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Auxiliary functions\n",
    "def risize_frame(frame, scale_percent):\n",
    "    \"\"\"Function to resize an image in a percent scale\"\"\"\n",
    "    width = int(frame.shape[1] * scale_percent / 100)\n",
    "    height = int(frame.shape[0] * scale_percent / 100)\n",
    "    dim = (width, height)\n",
    "\n",
    "    # resize image\n",
    "    resized = cv2.resize(frame, dim, interpolation = cv2.INTER_AREA)\n",
    "    return resized\n",
    "\n",
    "\n",
    "\n",
    "def filter_tracks(centers, patience):\n",
    "    \"\"\"Function to filter track history\"\"\"\n",
    "    filter_dict = {}\n",
    "    for k, i in centers.items():\n",
    "        d_frames = i.items()\n",
    "        filter_dict[k] = dict(list(d_frames)[-patience:])\n",
    "\n",
    "    return filter_dict\n",
    "\n",
    "\n",
    "def update_tracking(centers_old,obj_center, thr_centers, lastKey, frame, frame_max):\n",
    "    \"\"\"Function to update track of objects\"\"\"\n",
    "    is_new = 0\n",
    "    lastpos = [(k, list(center.keys())[-1], list(center.values())[-1]) for k, center in centers_old.items()]\n",
    "    lastpos = [(i[0], i[2]) for i in lastpos if abs(i[1] - frame) <= frame_max]\n",
    "    # Calculating distance from existing centers points\n",
    "    previous_pos = [(k,obj_center) for k,centers in lastpos if (np.linalg.norm(np.array(centers) - np.array(obj_center)) < thr_centers)]\n",
    "    # if distance less than a threshold, it will update its positions\n",
    "    if previous_pos:\n",
    "        id_obj = previous_pos[0][0]\n",
    "        centers_old[id_obj][frame] = obj_center\n",
    "    \n",
    "    # Else a new ID will be set to the given object\n",
    "    else:\n",
    "        if lastKey:\n",
    "            last = lastKey.split('D')[1]\n",
    "            id_obj = 'ID' + str(int(last)+1)\n",
    "        else:\n",
    "            id_obj = 'ID0'\n",
    "            \n",
    "        is_new = 1\n",
    "        centers_old[id_obj] = {frame:obj_center}\n",
    "        lastKey = list(centers_old.keys())[-1]\n",
    "\n",
    "    \n",
    "    return centers_old, id_obj, is_new, lastKey\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting People in ROI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-06T19:21:25.124997Z",
     "iopub.status.busy": "2023-03-06T19:21:25.12433Z",
     "iopub.status.idle": "2023-03-06T19:22:12.269Z",
     "shell.execute_reply": "2023-03-06T19:22:12.267749Z",
     "shell.execute_reply.started": "2023-03-06T19:21:25.124962Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] - Verbose during Prediction: False\n",
      "[INFO] - Original Dim:  (640, 480)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2e74349fc0b4dc0865a9b737e3712a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Configurations\n",
    "verbose = False\n",
    "scale_percent = 100\n",
    "conf_level = 0.8\n",
    "thr_centers = 20\n",
    "frame_max = 5\n",
    "patience = 100\n",
    "alpha = 0.1\n",
    "\n",
    "# Reading video with cv2\n",
    "video = cv2.VideoCapture(0)\n",
    "\n",
    "# Objects to detect (YOLO)\n",
    "class_IDS = [0]\n",
    "centers_old = {}\n",
    "obj_id = 0 \n",
    "end = []\n",
    "frames_list = []\n",
    "count_p = 0\n",
    "lastKey = ''\n",
    "print(f'[INFO] - Verbose during Prediction: {verbose}')\n",
    "\n",
    "# Original video information\n",
    "height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "fps = video.get(cv2.CAP_PROP_FPS)\n",
    "print('[INFO] - Original Dim: ', (width, height))\n",
    "\n",
    "# Scaling video for better performance\n",
    "if scale_percent != 100:\n",
    "    print('[INFO] - Scaling change may cause errors in pixel lines')\n",
    "    width = int(width * scale_percent / 100)\n",
    "    height = int(height * scale_percent / 100)\n",
    "    print('[INFO] - Dim Scaled: ', (width, height))\n",
    "    \n",
    "### Video output setup\n",
    "video_name = 'result.mp4'\n",
    "output_path = \"rep_\" + video_name\n",
    "tmp_output_path = \"tmp_\" + output_path\n",
    "VIDEO_CODEC = \"MP4V\"\n",
    "\n",
    "output_video = cv2.VideoWriter(tmp_output_path, \n",
    "                               cv2.VideoWriter_fourcc(*VIDEO_CODEC), \n",
    "                               fps, (width, height))\n",
    "\n",
    "# Executing Recognition \n",
    "for i in tqdm(range(int(video.get(cv2.CAP_PROP_FRAME_COUNT)))):\n",
    "    ret, frame = video.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Resize frame\n",
    "    frame = risize_frame(frame, scale_percent)\n",
    "    area_roi = [np.array([(1250, 400), (750, 400), (700, 800), (1200, 800)], np.int32)]\n",
    "    ROI = frame[390:800, 700:1300]\n",
    "\n",
    "    if verbose:\n",
    "        print('Dimension Scaled(frame): ', (frame.shape[1], frame.shape[0]))\n",
    "\n",
    "    # Getting predictions\n",
    "    y_hat = model.predict(ROI, conf=conf_level, classes=class_IDS, device=0, verbose=False)\n",
    "    \n",
    "    # Retrieve bounding boxes, confidence scores, and class labels\n",
    "    boxes = y_hat[0].boxes.xyxy.cpu().numpy()       # shape (n, 4)\n",
    "    conf = y_hat[0].boxes.conf.cpu().numpy()          # shape (n,)\n",
    "    classes = y_hat[0].boxes.cls.cpu().numpy()        # shape (n,)\n",
    "    \n",
    "    # Combine into a DataFrame\n",
    "    positions_frame = pd.DataFrame(\n",
    "        np.hstack((boxes, conf.reshape(-1, 1), classes.reshape(-1, 1))),\n",
    "        columns=['xmin', 'ymin', 'xmax', 'ymax', 'conf', 'class']\n",
    "    )\n",
    "    \n",
    "    # Translate numeric class labels to text (assuming dict_classes exists)\n",
    "    labels = [dict_classes[int(c)] for c in classes]\n",
    "    \n",
    "    # Process each detection\n",
    "    for ix, row in positions_frame.iterrows():\n",
    "        xmin, ymin, xmax, ymax, confidence, category = row[['xmin', 'ymin', 'xmax', 'ymax', 'conf', 'class']]\n",
    "        xmin, ymin, xmax, ymax = int(xmin), int(ymin), int(xmax), int(ymax)\n",
    "        \n",
    "        # Calculate center of the bounding box\n",
    "        center_x, center_y = int((xmin + xmax) / 2), int((ymin + ymax) / 2)\n",
    "        \n",
    "        # Update tracking for each object\n",
    "        centers_old, id_obj, is_new, lastKey = update_tracking(centers_old, (center_x, center_y), thr_centers, lastKey, i, frame_max)\n",
    "        \n",
    "        # Update count for new detections\n",
    "        count_p += is_new\n",
    "        \n",
    "        # Draw bounding box and centers on ROI\n",
    "        cv2.rectangle(ROI, (xmin, ymin), (xmax, ymax), (0, 0, 255), 2)\n",
    "        for cx, cy in centers_old[id_obj].values():\n",
    "            cv2.circle(ROI, (cx, cy), 5, (0, 0, 255), -1)\n",
    "        \n",
    "        # Draw label above bounding box\n",
    "        cv2.putText(ROI, f\"{id_obj}:{np.round(conf[ix], 2)}\", (xmin, ymin - 10),\n",
    "                    cv2.FONT_HERSHEY_TRIPLEX, 0.8, (0, 0, 255), 1)\n",
    "    \n",
    "    # Draw the count on the original frame\n",
    "    cv2.putText(frame, f'Counts People in ROI: {count_p}', (30, 40),\n",
    "                cv2.FONT_HERSHEY_TRIPLEX, 1.5, (255, 0, 0), 1)\n",
    "    \n",
    "    # Filter track history\n",
    "    centers_old = filter_tracks(centers_old, patience)\n",
    "    if verbose:\n",
    "        print(contador_in, contador_out)\n",
    "    \n",
    "    # Draw ROI area\n",
    "    overlay = frame.copy()\n",
    "    cv2.polylines(overlay, pts=area_roi, isClosed=True, color=(255, 0, 0), thickness=2)\n",
    "    cv2.fillPoly(overlay, area_roi, (255, 0, 0))\n",
    "    frame = cv2.addWeighted(overlay, alpha, frame, 1 - alpha, 0)\n",
    "    \n",
    "    # Save processed frame and write to video\n",
    "    frames_list.append(frame)\n",
    "    output_video.write(frame)\n",
    "    \n",
    "# Release video writer\n",
    "output_video.release()\n",
    "\n",
    "# Post processing: Fixing video output codec\n",
    "if os.path.exists(output_path):\n",
    "    os.remove(output_path)\n",
    "    \n",
    "subprocess.run(\n",
    "    [\"ffmpeg\", \"-i\", tmp_output_path, \"-crf\", \"18\", \"-preset\", \"veryfast\",\n",
    "     \"-hide_banner\", \"-loglevel\", \"error\", \"-vcodec\", \"libx264\", output_path]\n",
    ")\n",
    "os.remove(tmp_output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling Transformed Frames Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(frames_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-06T19:22:12.271381Z",
     "iopub.status.busy": "2023-03-06T19:22:12.270948Z",
     "iopub.status.idle": "2023-03-06T19:22:15.735467Z",
     "shell.execute_reply": "2023-03-06T19:22:15.734573Z",
     "shell.execute_reply.started": "2023-03-06T19:22:12.271308Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m [\u001b[32m62\u001b[39m,\u001b[32m63\u001b[39m, \u001b[32m64\u001b[39m, \u001b[32m65\u001b[39m, \u001b[32m66\u001b[39m]:\n\u001b[32m      3\u001b[39m     plt.figure(figsize =( \u001b[32m14\u001b[39m, \u001b[32m10\u001b[39m))\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     plt.imshow(\u001b[43mframes_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[32m      5\u001b[39m     plt.show()\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1400x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Checking samples of processed frames\n",
    "for i in [62,63, 64, 65, 66]:\n",
    "    plt.figure(figsize =( 14, 10))\n",
    "    plt.imshow(frames_list[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def show_webcam():\n",
    "    cap = cv2.VideoCapture(\"rtsp://admin:ZAMOUF@192.168.0.137:554/H.264\")  # Open the default webcam\n",
    "    if not cap.isOpened():\n",
    "        print(\"Cannot open webcam\")\n",
    "        return\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()  # Read frame from webcam\n",
    "        if not ret:\n",
    "            print(\"Can't receive frame. Exiting...\")\n",
    "            break\n",
    "\n",
    "        cv2.imshow('Webcam Feed', frame)  # Display the resulting frame\n",
    "\n",
    "        # Exit when 'q' key is pressed\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()  # Release the capture\n",
    "    cv2.destroyAllWindows()  # Close any open windows\n",
    "\n",
    "show_webcam()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executing Result Video "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPU:  1\n",
      "GPU Name:  NVIDIA GeForce RTX 3060 Laptop GPU\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"Number of GPU: \", torch.cuda.device_count())\n",
    "print(\"GPU Name: \", torch.cuda.get_device_name())\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-06T19:22:15.737813Z",
     "iopub.status.busy": "2023-03-06T19:22:15.736837Z",
     "iopub.status.idle": "2023-03-06T19:22:15.963732Z",
     "shell.execute_reply": "2023-03-06T19:22:15.962389Z",
     "shell.execute_reply.started": "2023-03-06T19:22:15.73776Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video controls  width=\"896\"  height=\"503\">\n",
       " <source src=\"data:None;base64,rep_result.mp4\" type=\"None\">\n",
       " Your browser does not support the video tag.\n",
       " </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#output video result\n",
    "frac = 0.7 \n",
    "Video(data='rep_result.mp4', embed=True, height=int(720 * frac), width=int(1280 * frac))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30387,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "YOLO",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
